---
layout: default
title: Graph neural networks
description: Description of talks on November 9th, 2023
---

## Explainable Graph Machine Learning : Challenges and Solutions

**Speaker**: [Megha Khosla](https://khosla.github.io/)

**Abstract**: Graph-based machine learning (GraphML) has led to state of the art  improvements in various scientific tasks, yet its opaque nature hinders its full potential in sensitive domains. In this talk, I will commence by providing a brief overview of prevalent notions of post-hoc explanations for GraphML techniques, shedding light on the intricacies of finding and evaluating these explanations. I will present our proposed effective solutions and conclude by exploring intriguing problems that continue to stimulate research in this domain.

**Recording**: [Here](https://www.youtube.com/watch?v=J5UDZ3Ln7ac&list=PLdzxeAZte_YvxvSCBiQyDdyO8muNHWnLr&index=6)



## Multi-label Node Classification On Graph-Structured Data


**Speaker**: [Tianqi Zhao](https://scholar.google.com/citations?user=g09j58gAAAAJ&hl=en)

**Abstract**: Graph Neural Networks (GNNs) have exhibited remarkable progress in node classification on graphs, particularly in a multi-class setting. However, their applicability to the more realistic multi-label classification scenario, where nodes can have multiple labels, has been largely overlooked. This talk will unveil the limitations of current GNNs in handling multi-label classification tasks. I will also highlight how existing GNNs, designed for either homophilic or heterophilic characteristics of node labels, fall short in capturing the nuanced complexities of multi-label datasets that don't conform to such clear distinctions.

**Recording**: [Here](https://www.youtube.com/watch?v=eGjqgAKEf1o&list=PLdzxeAZte_YvxvSCBiQyDdyO8muNHWnLr&index=7)



## Self-Attention Message Passing for Contrastive Few-Shot Image Classification


**Speaker**: [Ojas Shirekar](https://scholar.google.com/citations?user=faROrSgAAAAJ&hl=en)

**Abstract**: Humans have a unique ability to learn new representations from just a handful of examples with little to no supervision. Deep learning models, however, require an abundance of data and supervision to perform at a satisfactory level. Unsupervised few-shot learning (U-FSL) is the pursuit of bridging this gap between machines and humans. Inspired by the capacity of graph neural networks (GNNs) in discovering complex inter-sample relationships, we propose a novel self-attention based message passing contrastive learning approach (coined as SAMP-CLR) for U-FSL pre-training. This work also proposes an optimal transport (OT) based fine-tuning strategy (called OpT-Tune) to efficiently induce task awareness into our novel end-to-end unsupervised few-shot classification framework (SAMPTransfer).



**Recording**: [Here](https://www.youtube.com/watch?v=1RyKfa002z8&list=PLdzxeAZte_YvxvSCBiQyDdyO8muNHWnLr&index=6&ab_channel=Elvinisufi)





[back](../)
