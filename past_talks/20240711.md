---
layout: default
title: Special session
description: Description of talks on July 11th, 2024
---

## Learning on directed acyclic graphs.

**Speaker**: Samuel Rey Escudero

**Abstract**: Directed acyclic graphs are a particular type of graph whose lack of cycles renders them well-suited to model complex relations between pairs of variables such as ordering, hierarchy, or causality. However, learning the topology of a DAG from data classically involves solving a non-trivial combinatorial problem to ensure the acyclicity of the learned graph. Moreover, even when the DAG is known a priori, the nilpotent adjacency matrices pose unique challenges in developing DAG signal processing and machine learning tools. To address the first challenge, recent works have introduced a method to recast the original combinatorial problem into a non-convex optimization problem involving only continuous constraints. Then, by assuming additional structure about the topology of the DAG and signals sampled from a linear structural equation model, we introduce a convex relaxation of the acyclicity that guarantees the recovery of the optimum DAG. Regarding the challenges inherent to the processing of signals defined over DAGs, we harness recent advances offering alternative definitions of causal shifts and convolutions for signals on DAGs. We develop a novel convolutional graph neural network that integrates learnable DAG filters to account for the partial ordering induced by the graph topology, thus providing valuable inductive bias to learn effective representations of DAG-supported data. We discuss the salient advantages and potential limitations of the proposed DAG convolutional network.



## Online learning of nonlinear and dynamic graphs 


**Speaker**: Rohan Thekkemarickal Money

**Abstract**: Multivariate time series data is generated by various real-world networks, including large-scale cyber-physical systems (CPS), financial networks, and brain networks. Inferring and exploiting the hidden graph structure of this data can aid various inference tasks and provide valuable insights into the system. Learning the graph from such complex systems is challenging due to their nonlinear and dynamic nature. We propose novel algorithms for learning nonlinear graphs from streaming data. These methods use kernel techniques to address nonlinearity, and the optimization problem is formulated in a node-separable manner, significantly enhancing the algorithm's scalability. The curse of dimensionality associated with kernel methods is tackled using random feature approximation (RF), which additionally provides nodal data privacy. Additionally, we provide theoretical guarantees in terms of dynamic regret, which characterize the tracking capability of an online algorithm.
 

## Mixup for augmenting data in myriad scenarios

 
**Speaker**: Madeline Navarro


**Abstract:** Mixup is a cheap and efficient data augmentation approach for generating synthetic data. New samples are created through convex combinations of pairs of existing labeled samples. Inspired by the empirical and theoretical success of mixup, we provide extensions to the mixup method in three directions: i) beyond linear pairwise mixup, we present an approach to perform mixup on multiple samples via convex clustering, ii) beyond Euclidean data, we perform mixup on networks by considering limit objects of such discrete structures, iii) beyond improving accuracy, we take a novel approach to apply mixup data augmentation to improve model fairness. We empirically validate our three directions in several benchmark datasets. We conclude by outlining future work for each of our directions of mixup development.

## Exploiting the Structure of Two Graphs with Graph Neural Networks


**Speaker**: Víctor Manuel Tenorio Gómez


**Abstract**: As the volume and complexity of current datasets continue to increase, there is an urgent need to develop new deep learning architectures that can handle such data efficiently. Graph neural networks (GNNs) have emerged as a promising solution to deal with unstructured data, outperforming traditional deep learning architectures. However, most of the current GNN models are designed to work with a single graph, which limits their applicability in many real-world scenarios where multiple graphs may be involved. To address this limitation, we propose a novel framework that enables GNNs to handle tasks where the input is represented as a signal on top of one graph (the input graph) and the output is a graph signal defined over a different graph (the output graph). This is achieved by first processing the input data using a GNN that operates over the input graph, then applying a transformation function, and finally implementing a different GNN that uses the output graph. Our framework is not intended to propose a single architecture but rather to provide a flexible approach to solve such tasks. By leveraging information from multiple graphs, our framework can capture more complex relationships between different entities in the data, leading to improved performance in a wide range of applications. We also analyze the setup where the focus is not on the output space, but on the underlying latent space and, inspired by Canonical Correlation Analysis, we seek informative representations of the data that can be leveraged to solve a downstream task. We test the architecture in several experimental setups using real world datasets, and we observe that the proposed architecture works better than traditional deep learning architectures, showcasing the importance of leveraging the information of the two graphs involved in the task.




[back](../)
