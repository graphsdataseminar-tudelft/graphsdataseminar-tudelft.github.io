---
layout: default
title: Special session - Learning with graphs
description: Description of talks on July 11th, 2024
---



## Online learning of nonlinear and dynamic graphs 


**Speaker**: [Rohan Thekkemarickal Money](https://scholar.google.com/citations?user=JnkbP8EAAAAJ&hl=en)

**Abstract**: Multivariate time series data is generated by various real-world networks, including large-scale cyber-physical systems (CPS), financial networks, and brain networks. Inferring and exploiting the hidden graph structure of this data can aid various inference tasks and provide valuable insights into the system. Learning the graph from such complex systems is challenging due to their nonlinear and dynamic nature. We propose novel algorithms for learning nonlinear graphs from streaming data. These methods use kernel techniques to address nonlinearity, and the optimization problem is formulated in a node-separable manner, significantly enhancing the algorithm's scalability. The curse of dimensionality associated with kernel methods is tackled using random feature approximation (RF), which additionally provides nodal data privacy. Additionally, we provide theoretical guarantees in terms of dynamic regret, which characterize the tracking capability of an online algorithm.

[ [<span style="color:#D22B2B">Slides</span>
](../slides/20240711_money.pdf) ] [ [<span style="color:green">Recording</span>
](https://www.youtube.com/watch?v=3QFQ1rdJhH4&ab_channel=Elvinisufi) ]


## Mixup for augmenting data in myriad scenarios

 
**Speaker**: [Madeline Navarro](https://scholar.google.com/citations?user=LJxDdfMAAAAJ&hl=en)


**Abstract:** Mixup is a cheap and efficient data augmentation approach for generating synthetic data. New samples are created through convex combinations of pairs of existing labeled samples. Inspired by the empirical and theoretical success of mixup, we provide extensions to the mixup method in three directions: i) beyond linear pairwise mixup, we present an approach to perform mixup on multiple samples via convex clustering, ii) beyond Euclidean data, we perform mixup on networks by considering limit objects of such discrete structures, iii) beyond improving accuracy, we take a novel approach to apply mixup data augmentation to improve model fairness. We empirically validate our three directions in several benchmark datasets. We conclude by outlining future work for each of our directions of mixup development.

[ [<span style="color:#D22B2B">Slides</span>
](../slides/20240711_navarro.pdf) ] [ [<span style="color:green">Recording</span>
](https://www.youtube.com/watch?v=YtEAyeUZk9g&ab_channel=Elvinisufi) ]


## Exploiting the Structure of Two Graphs with Graph Neural Networks


**Speaker**: [Víctor Manuel Tenorio Gómez](https://gestion2.urjc.es/pdi/ver/victor.tenorio)


**Abstract**: As the volume and complexity of current datasets continue to increase, there is an urgent need to develop new deep learning architectures that can handle such data efficiently. Graph neural networks (GNNs) have emerged as a promising solution to deal with unstructured data, outperforming traditional deep learning architectures. However, most of the current GNN models are designed to work with a single graph, which limits their applicability in many real-world scenarios where multiple graphs may be involved. To address this limitation, we propose a novel framework that enables GNNs to handle tasks where the input is represented as a signal on top of one graph (the input graph) and the output is a graph signal defined over a different graph (the output graph). This is achieved by first processing the input data using a GNN that operates over the input graph, then applying a transformation function, and finally implementing a different GNN that uses the output graph. Our framework is not intended to propose a single architecture but rather to provide a flexible approach to solve such tasks. By leveraging information from multiple graphs, our framework can capture more complex relationships between different entities in the data, leading to improved performance in a wide range of applications. We also analyze the setup where the focus is not on the output space, but on the underlying latent space and, inspired by Canonical Correlation Analysis, we seek informative representations of the data that can be leveraged to solve a downstream task. We test the architecture in several experimental setups using real world datasets, and we observe that the proposed architecture works better than traditional deep learning architectures, showcasing the importance of leveraging the information of the two graphs involved in the task.

[ [<span style="color:#D22B2B">Slides</span>
](../slides/20240711_tenorio.pdf) ] [ [<span style="color:green">Recording</span>
](https://www.youtube.com/watch?v=KXfvu2maXEM&ab_channel=Elvinisufi) ]



[back](../index.md#july-11th-2024-special-session)
